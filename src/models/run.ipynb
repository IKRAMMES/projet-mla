{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import Decoder\n",
    "from decoderattn import Attention_Decoder\n",
    "from encoder import Encoder\n",
    "from evaluatemodel import MySeq2SeqModelEvaluation\n",
    "from train_with_batch import Seq2SeqTrainer\n",
    "from trainmodel import Seq2SeqTrainer\n",
    "from data_proc import DataPreparation\n",
    "from data_proc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from mosestokenizer import *\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "max_length = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Form:  ['l or a . dollars l once ?', ' gold ?']\n",
      "Read 2000 sentence pairs\n",
      "Trimmed to 1837 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 6924\n",
      "fra 6019\n"
     ]
    }
   ],
   "source": [
    "obj= DataPreparation('fra', 'eng', True)\n",
    "input_lang, output_lang, pairs= obj.prepare_data(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "attn_decoder = Attention_Decoder(hidden_size, output_lang.n_words, max_length , dropoutP=0.1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#encoder1 = EncoderRNN(input_dim = input_lang.n_words,emb_dim =256,enc_hid_dim = 1000, dec_hid_dim = 1000, dropout = 0.6) \n",
    "encoder = Encoder(input_size = input_lang.n_words, hidden_size = hidden_size, device = device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from data_proc import *\n",
    "\n",
    "from decoderattn import Attention_Decoder\n",
    "from decoder import Decoder\n",
    "\n",
    "\n",
    "class Seq2SeqTrainer:\n",
    "    def __init__(self, encoder_model, decoder_model, max_sequence_length, start_token, end_token, input_lang ,output_lang , device):\n",
    "        self.encoder_model = encoder_model\n",
    "        #self.decoder_type = decoder_type\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.device = device\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.decoder_model= decoder_model\n",
    "\n",
    "        '''\n",
    "        if self.decoder_type == 'simple':\n",
    "            self.decoder_model = Decoder(encoder_model.hidden_size * 2, decoder_model.output_size)\n",
    "        elif self.decoder_type == 'attention':\n",
    "            self.decoder_model = Attention_Decoder(encoder_model.hidden_size, decoder_model.output_size, max_sequence_length)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid decoder type. Supported types are 'simple' and 'attention'.\")'''\n",
    "\n",
    "    def train(self, input_sequence, target_sequence, encoder_optimizer, decoder_optimizer, loss_criterion, probability=0.5):\n",
    "        # Initializations\n",
    "        input_len = input_sequence.size(0)\n",
    "        target_len = target_sequence.size(0)\n",
    "        total_loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_hidden_states = torch.zeros(self.max_sequence_length, self.encoder_model.hidden_size * 2, device=self.device)\n",
    "\n",
    "        # Initialize the decoder\n",
    "        current_input = torch.tensor([[self.start_token]], device=self.device)\n",
    "\n",
    "        # Encoding Phase\n",
    "        hidden_state = self.encoder_model.init_hidden()\n",
    "\n",
    "        for i in range(input_len):\n",
    "            encoder_output, hidden_state = self.encoder_model(input_sequence[i])\n",
    "            encoder_hidden_states[i] = encoder_output[0, 0]\n",
    "\n",
    "            # Decoder takes the last hidden state of the encoder\n",
    "            hidden_state_decoder = hidden_state\n",
    "\n",
    "            # Decoding Phase\n",
    "            for j in range(target_len):\n",
    "                decoder_output, hidden_state_decoder, _ = self.decoder_model(current_input, hidden_state_decoder, encoder_hidden_states)\n",
    "                total_loss += loss_criterion(decoder_output, target_sequence[j])\n",
    "\n",
    "                explore_decision = (np.random.rand() < probability)\n",
    "                if explore_decision:\n",
    "                    current_input = target_sequence[j]\n",
    "                else:\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    current_input = topi.squeeze().detach()\n",
    "\n",
    "                if current_input.item() == self.end_token:\n",
    "                    break\n",
    "\n",
    "        # Backpropagation and weight update\n",
    "        total_loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        return total_loss.item() / target_len\n",
    "\n",
    "    def custom_training_function(self, num_epochs, print_interval=500, pairs=0, plot_interval=50, learning_rate=0.01):\n",
    "        # Initialize loss tracking variables\n",
    "        all_losses = []\n",
    "        current_loss_sum = 0\n",
    "        plot_loss_sum = 0\n",
    "\n",
    "        # Initialize Adam optimizers for the two networks with the specified learning rate\n",
    "        optimizer_encoder = optim.Adam(self.encoder_model.parameters(), lr=learning_rate)\n",
    "        optimizer_decoder = optim.Adam(self.decoder_model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Generate training data using random pairs and define the loss criterion\n",
    "        training_data = [[tensorsFromPair(random.choice(pairs) , self.input_lang,self.output_lang,self.device)] for _ in range(num_epochs)]\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            data_point = training_data[epoch - 1]\n",
    "            input_data = data_point[0]\n",
    "            target_data = data_point[1]\n",
    "\n",
    "            loss = self.train(input_data, target_data, optimizer_encoder, optimizer_decoder, nn.NLLLoss())\n",
    "            current_loss_sum += loss\n",
    "            plot_loss_sum += loss\n",
    "\n",
    "            # Print the average loss at specified intervals\n",
    "            if epoch % print_interval == 0:\n",
    "                avg_print_loss = current_loss_sum / print_interval\n",
    "                current_loss_sum = 0\n",
    "                print('%s (%d %d%%) %.4f' % (self.calculate_time_elapsed(time.time(), epoch / num_epochs),\n",
    "                                             epoch, epoch / num_epochs * 100, avg_print_loss))\n",
    "\n",
    "            # Track losses for plotting at specified intervals\n",
    "            if epoch % plot_interval == 0:\n",
    "                avg_plot_loss = plot_loss_sum / plot_interval\n",
    "                all_losses.append(avg_plot_loss)\n",
    "                plot_loss_sum = 0\n",
    "\n",
    "        # Print the list of losses for visualization\n",
    "        print(\"Loss Visualization: \", all_losses)\n",
    "\n",
    "    # Function to calculate elapsed and remaining time\n",
    "    def calculate_time_elapsed(self, start, progress):\n",
    "        elapsed_seconds = time.time() - start\n",
    "        remaining_seconds = (elapsed_seconds / progress) * (1 - progress)\n",
    "        return f'Time Elapsed: {int(elapsed_seconds)}s, Remaining: {int(remaining_seconds)}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence, device):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair,input_lang,output_lang, device):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0],device)\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1],device)\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, lang1, lang2, reverse=False):\n",
    "        #self.input_lang, self.output_lang, self.pairs = self.prepare_data(reverse)\n",
    "        self.lang1=lang1\n",
    "        self.lang2=lang2\n",
    "        self.reverse=reverse\n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        return s\n",
    "\n",
    "    def read_langs(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "\n",
    "        # Read the file and split into lines\n",
    "        lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "        # Split every line into pairs and normalize\n",
    "        pairs = [[self.normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "        # Reverse pairs, make Lang instances\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_lang = Lang(lang2)\n",
    "            output_lang = Lang(lang1)\n",
    "        else:\n",
    "            input_lang = Lang(lang1)\n",
    "            output_lang = Lang(lang2)\n",
    "\n",
    "        return input_lang, output_lang, pairs\n",
    "\n",
    "    def filter_pair(self, p):\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "               len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair)]\n",
    "\n",
    "    def prepare_data(self, reverse=True):\n",
    "        self.input_lang, self.output_lang, pairs = self.read_langs(self.lang1, self.lang2, reverse)\n",
    "        print(\"Form: \", pairs[0])\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in pairs:\n",
    "            self.input_lang.add_sentence(pair[0])\n",
    "            self.output_lang.add_sentence(pair[1])\n",
    "        print(\"Counted words:\")\n",
    "        print(self.input_lang.name, self.input_lang.n_words)\n",
    "        print(self.output_lang.name, self.output_lang.n_words)\n",
    "        return self.input_lang, self.output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb Cellule 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m TRAIN1\u001b[39m=\u001b[39mSeq2SeqTrainer(encoder, attn_decoder, max_length, start_token, end_token,input_lang ,output_lang , device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m custom_train1\u001b[39m=\u001b[39mTRAIN1\u001b[39m.\u001b[39;49mcustom_training_function(num_epochs, \u001b[39m500\u001b[39;49m, pairs, plot_interval\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32m/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb Cellule 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m data_point \u001b[39m=\u001b[39m training_data[epoch \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m input_data \u001b[39m=\u001b[39m data_point[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m target_data \u001b[39m=\u001b[39m data_point[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(input_data, target_data, optimizer_encoder, optimizer_decoder, nn\u001b[39m.\u001b[39mNLLLoss())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/NEw/MLA_PROJET_NEW/projet-mla-4/src/models/run.ipynb#W6sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m current_loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "start_token = 0\n",
    "end_token = 1\n",
    "max_length = 50\n",
    "num_epochs = 1\n",
    "TRAIN1=Seq2SeqTrainer(encoder, attn_decoder, max_length, start_token, end_token,input_lang ,output_lang , device)\n",
    "custom_train1=TRAIN1.custom_training_function(num_epochs, 500, pairs, plot_interval=50, learning_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
